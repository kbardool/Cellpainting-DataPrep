{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Build NN model to predict TPSA from Pharmacophores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:25.298115Z",
     "start_time": "2023-12-21T20:50:24.855884Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "%load_ext autoreload  \n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:28.460258Z",
     "start_time": "2023-12-21T20:50:25.868966Z"
    },
    "execution": {
     "iopub.execute_input": "2023-12-21T22:06:03.483275Z",
     "iopub.status.busy": "2023-12-21T22:06:03.483000Z",
     "iopub.status.idle": "2023-12-21T22:06:05.575074Z",
     "shell.execute_reply": "2023-12-21T22:06:05.574493Z",
     "shell.execute_reply.started": "2023-12-21T22:06:03.483240Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbardool/miniconda3/envs/cellpainting/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 10.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import itertools\n",
    "from collections.abc import Iterator\n",
    "from   datetime import datetime\n",
    "from pprint import PrettyPrinter\n",
    "import joblib\n",
    "\n",
    "from utils import *\n",
    "from utils_ml import model_selection\n",
    "\n",
    "from multiprocessing import Pool, process\n",
    "\n",
    "# from ydata_profiling import ProfileReport\n",
    "\n",
    "import dask.dataframe as dd \n",
    "pp = PrettyPrinter(indent=4)\n",
    "np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "pd.options.display.width = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:28.476862Z",
     "start_time": "2023-12-21T20:50:28.461907Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Adashare_Train.ipynb\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:28.560656Z",
     "start_time": "2023-12-21T20:50:28.478740Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:28.560656Z",
     "start_time": "2023-12-21T20:50:28.478740Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "\n",
    "# utility class to help normalize labels such that they contain only values between 0 and n_classes-1. \n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer, normalize, MaxAbsScaler,MinMaxScaler\n",
    "\n",
    "#K-Folds cross-validator - Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).\n",
    "# Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Evaluate metric(s) by cross-validation and also record fit/score times.\n",
    "from sklearn.model_selection import cross_validate, cross_val_score \n",
    "\n",
    "# Generate cross-validated estimates for each input data point.\n",
    "# The data is split according to the cv parameter. Each sample belongs to exactly one test set, \n",
    "# and its prediction is computed with an estimator fitted on the corresponding training set.\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Exhaustive search over specified parameter values for an estimator.\n",
    "# Randomized search on hyper parameters.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
    "\n",
    "#Input checker utility for building a cross-validator.(?)\n",
    "\n",
    "from sklearn.model_selection._split import check_cv\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Make a scorer from a performance metric or loss function. This factory function wraps scoring \n",
    "# functions for use in GridSearchCV and cross_val_score. It takes a score function, such as accuracy_score, \n",
    "# mean_squared_error, adjusted_rand_score or average_precision_score and returns a callable that \n",
    "# scores an estimatorâ€™s output. The signature of the call is (estimator, X, y) where estimator\n",
    "# is the model to be evaluated, X is the data and y is the ground truth labeling (or None in the\n",
    "# case of unsupervised models).\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "#regression matrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error , mean_squared_error, r2_score\n",
    "\n",
    "#classification metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# Construct a new unfitted estimator with the same parameters. Clone does a deep copy of the model \n",
    "# in an estimator without actually copying attached data. It returns a new estimator with the same \n",
    "# parameters that has not been fitted on any data.\n",
    "\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:31.139023Z",
     "start_time": "2023-12-21T20:50:30.891155Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask_ml.model_selection as dcv\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.model_selection import GridSearchCV, IncrementalSearchCV, HyperbandSearchCV\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import LocalCluster\n",
    "import joblib\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "# from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:32.339732Z",
     "start_time": "2023-12-21T20:50:32.323764Z"
    }
   },
   "outputs": [],
   "source": [
    "def result_model_selection(results, name):\n",
    "    df_results = pd.DataFrame({'model'     : [name] * len(results.cv_results_['params']),\n",
    "                               'params'    : results.cv_results_['params'],\n",
    "                               'mean score': results.cv_results_['mean_test_score'],\n",
    "                               'std score' : results.cv_results_['std_test_score'],\n",
    "                               'rank'      : results.cv_results_['rank_test_score']\n",
    "                              })\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T22:06:06.710672Z",
     "iopub.status.busy": "2023-12-21T22:06:06.710128Z",
     "iopub.status.idle": "2023-12-21T22:06:06.713715Z",
     "shell.execute_reply": "2023-12-21T22:06:06.713263Z",
     "shell.execute_reply.started": "2023-12-21T22:06:06.710653Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_cv_splits(n_folds: int = 5,) -> Iterator[tuple[dd.DataFrame, dd.DataFrame]]:\n",
    "    frac = [1 / n_folds] * n_folds\n",
    "    splits = ddf.random_split(frac, shuffle=True)\n",
    "    for i in range(n_folds):\n",
    "        train = [splits[j] for j in range(n_folds) if j != i]\n",
    "        test = splits[i]\n",
    "        yield dd.concat(train), test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:34.752816Z",
     "start_time": "2023-12-21T20:50:34.052537Z"
    }
   },
   "outputs": [],
   "source": [
    "gzip_compression_options = {\"method\": \"gzip\", 'compresslevel': 1,\"mtime\": 1}\n",
    "\n",
    "metadata_path = \"../cj-datasets/metadata\"\n",
    "plates = pd.read_csv(os.path.join(metadata_path,\"plate.csv.gz\"))\n",
    "wells = pd.read_csv(os.path.join(metadata_path,\"well.csv.gz\"))\n",
    "compound = pd.read_csv(os.path.join(metadata_path,\"compound.csv.gz\"))\n",
    "orf = pd.read_csv(os.path.join(metadata_path,\"orf.csv.gz\"))\n",
    "\n",
    "\n",
    "## gz, bz2, zip, tar, tar.gz, tar.bz2\n",
    "# types = ['.gz', '.bz2','.zip', '.tar', '.tar.gz', '.tar.bz2']\n",
    "type_bz2 = 'bz2'\n",
    "type_gzip = 'gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:35.620802Z",
     "start_time": "2023-12-21T20:50:35.605234Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix = '' ### Target-2' , 'MOA'\n",
    "\n",
    "input_path =\"./input/\"\n",
    "output_path =\"./output_11102023/\"\n",
    "\n",
    "prefix_lc = prefix.lower().replace('-', '_')\n",
    " \n",
    "# compoundMetadataInputFile   = f\"{input_path}JUMP-{prefix}_compound_library.csv\"\n",
    "\n",
    "compoundMetadataInputFile          = f\"{output_path}{prefix_lc}_compound_metadata.csv\"\n",
    "compoundMetadataCleanFile          = f\"{output_path}{prefix_lc}_compound_metadata_clean.csv\"\n",
    "compoundMetadataTPSAFile           = f\"{output_path}{prefix_lc}_compound_metadata_tpsa.csv\"\n",
    "compoundMetadataTPSACleanFile      = f\"{output_path}{prefix_lc}_compound_metadata_tpsa_clean.csv\"\n",
    "compoundTPSAFile                   = f\"{output_path}{prefix_lc}_compound_TPSA.csv\"\n",
    "compoundTPSACleanFile              = f\"{output_path}{prefix_lc}_compound_TPSA_clean.csv\"\n",
    "compoundPharmacophoreFile          = f\"{output_path}{prefix_lc}_compound_pharmacophores_sparse.pkl\"\n",
    "compoundPharmacophoreCleanFile     = f\"{output_path}{prefix_lc}_compound_pharmacophores_sparse_clean.pkl\"\n",
    "compoundPharmacophoreDenseZipFile  = f\"{output_path}{prefix_lc}_compound_pharmacophores_dense.npz\"\n",
    "CompoundExtendedMetadataFile        = f\"{output_path}{prefix_lc}compound_extended_metadata.csv\"\n",
    "\n",
    "CompoundExtendedMetadata5SampleFile = f\"{output_path}{prefix_lc}compound_extended_metadata_5samples.csv\"\n",
    "CompoundProfiles5SampleFileCSV      = f\"{output_path}{prefix_lc}compound_profiles_5samples.csv\"\n",
    "\n",
    "CompoundExtendedMetadata2SampleFile = f\"{output_path}{prefix_lc}compound_extended_metadata_2samples.csv\"\n",
    "CompoundProfiles2SampleFileCSV      = f\"{output_path}{prefix_lc}compound_profiles_2samples.csv\"\n",
    "\n",
    "# CompoundProfiles2SampleFileParquet  = f\"{output_path}{prefix_lc}compound_profiles_2samples.parquet\"\n",
    "# CompoundProfiles2SampleFileGZ       = f\"{output_path}{prefix_lc}compound_profiles_2samples.gz\"\n",
    "\n",
    "CompoundExtendedMetadataSampleFile  = f\"{output_path}{prefix_lc}compound_extended_metadata_samples.csv\"\n",
    "featureSelectionFile                = f\"{output_path}{prefix_lc}_normalized_feature_select.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:35.940324Z",
     "start_time": "2023-12-21T20:50:35.923756Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\" \")\n",
    "print(f\" compound Metadata Input File             : {compoundMetadataInputFile}\")\n",
    "print(f\" compound Metadata Output File            : {compoundMetadataCleanFile}\")\n",
    "print(f\" compound Metadata + TPSA File            : {compoundMetadataTPSAFile}\")\n",
    "print(f\" compound Metadata + TPSA Cleaned File    : {compoundMetadataTPSACleanFile}\\n\")\n",
    "print(f\" compound TPSA File                       : {compoundTPSAFile}\")\n",
    "print(f\" compound TPSA Clean File                 : {compoundTPSACleanFile}\")\n",
    "print()\n",
    "print(f\" compound Pharmacophore Sparse File       : {compoundPharmacophoreFile}\")\n",
    "print(f\" compound Pharmacophore Sparse Clean File : {compoundPharmacophoreCleanFile}\")\n",
    "print(f\" compound Pharmacophore Dense Zipped File : {compoundPharmacophoreDenseZipFile}\")\n",
    "print(f\" \")\n",
    "print(f\" Compound Extended MetadataFile           : {CompoundExtendedMetadataFile }\")\n",
    "print(f\" Compound Extended Metadata 5 SampleFile  : {CompoundExtendedMetadata5SampleFile }\")\n",
    "print(f\" Compound Profiles 5 Sample File CSV      : {CompoundProfiles5SampleFileCSV }\")\n",
    "print()\n",
    "print(f\" Compound Extended Metadata 2 SampleFile  : {CompoundExtendedMetadata2SampleFile }\")\n",
    "print(f\" Compound Profiles 2 Samples File CSV     : {CompoundProfiles2SampleFileCSV}\")\n",
    "# print(f\" \")\n",
    "# print(f\" CompoundProfiles2SamplesFile Parquet        : {CompoundProfiles2SampleFileParquet }\")\n",
    "# print(f\" CompoundProfiles2SamplesFile gz             : {CompoundProfiles2SampleFileGZ }\")\n",
    "print(f\" \")\n",
    "print(f\" featureSelectionFile                     : {featureSelectionFile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load and Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dask cluster and client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:41.337029Z",
     "start_time": "2023-12-21T20:50:40.947431Z"
    }
   },
   "outputs": [],
   "source": [
    "# cluster = LocalCluster()\n",
    "cluster = LocalCluster(\"Kevins_Cluster\", n_workers=2, threads_per_worker=2)\n",
    "# client = Client(\"tcp://127.0.0.1:37937\")\n",
    "client = Client(cluster.scheduler_address)\n",
    "# client = Client(processes = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.workers\n",
    "# cluster.scale(2)\n",
    "cluster.close()\n",
    "client.close()\n",
    "# del cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# client\n",
    "# cluster.name\n",
    "# print(cluster)\n",
    "# cluster.dashboard_link\n",
    "# cluster.scheduler_address\n",
    "cluster.scheduler_spec\n",
    "# cluster.workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.scheduler.stop()\n",
    "# cluster.scheduler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client \n",
    "# client.status\n",
    "# client.connection_args\n",
    "# del client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./metadata/parquet_columns.pkl\",'rb') as f:\n",
    "#     ParquetColumns = pickle.load(f)\n",
    "\n",
    "# for k,v in ParquetColumns.items():\n",
    "#     print(f\" {k:20s}   items: {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ParquetColumns['Cells']['Cells_AreaShape_Area'])\n",
    "# ParquetColumns['Cells']\n",
    "# del ParquetColumns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read column metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:50:55.041400Z",
     "start_time": "2023-12-21T20:50:55.011436Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./metadata/feature_selection_columns.pkl\", 'rb') as f: \n",
    "    x = pickle.load(f)\n",
    "for i in x:\n",
    "    print(f\" {i:20s}    {len(x[i])} \")\n",
    "\n",
    "X_columns = [] ## [\"Metadata_JCP2022\"]\n",
    "y_columns = [] ## [\"Metadata_JCP2022\"]\n",
    "X_columns.extend(x['selected'])\n",
    "y_columns.extend([ \"Metadata_log10TPSA\"])\n",
    " \n",
    "\n",
    "x_columns_drop = [\"Metadata_Source\", \"Metadata_Batch\", \"Metadata_Plate\", \"Metadata_Well\", \"Metadata_TPSA\", \"Metadata_lnTPSA\", \"Metadata_log10TPSA\"]\n",
    "# x_columns_drop.extend([\"Metadata_JCP2022\"])\n",
    "\n",
    "# columns_read = [\"Metadata_JCP2022\", \"Metadata_log10TPSA\"]\n",
    "# print(f\" len(columns_read) : {len(columns_read)}\")\n",
    "print(f\" len(x_columms)    : {len(X_columns)}\")\n",
    "print(f\" len(y_columms)    : {len(y_columns)}\")\n",
    "\n",
    "# columns_read.extend(x['selected'])\n",
    "# print(f\" len(columns_read) : {len(columns_read)}\")\n",
    "x_columns_dtype = {x: np.dtype('float32') for x in X_columns}\n",
    "y_columns_dtype = {x: np.dtype('float32') for x in y_columns} ## \"Metadata_log10TPSA\":np.dtype('float64')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read compound profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply feature selection\n",
    "profilesFile = CompoundProfiles2SampleFileCSV ## +'.'+ type_bz2\n",
    "featureSelectionFile = './output_11102023//normalized_feature_select.csv'\n",
    "\n",
    "print(f\" Profiles file       :  {profilesFile}\")\n",
    "print(f\" Features select file:  {featureSelectionFile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:38:33.906700Z",
     "start_time": "2023-07-31T18:38:33.684133Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_profiles = dd.read_csv(profilesFile, blocksize=\"100MB\", usecols=columns_read)  ##, index_col = 'CASRN')\n",
    "\n",
    "# df_profiles.info()\n",
    "# df_profiles.head(6)\n",
    "# del df_X\n",
    "# del df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:38:33.906700Z",
     "start_time": "2023-07-31T18:38:33.684133Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_X = dd.read_csv(profilesFile, blocksize=\"100MB\", usecols=X_columns, dtype= x_columns_dtype)  ##, index_col = 'CASRN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_X.info()\n",
    "# df_X.head()\n",
    "# df_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:38:33.906700Z",
     "start_time": "2023-07-31T18:38:33.684133Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_y = dd.read_csv(profilesFile, blocksize=\"100MB\", usecols=y_columns, dtype=y_columns_dtype)  ##, index_col = 'CASRN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_y_array.info()\n",
    "# df_y_array.head()\n",
    "# df_y_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_X_array = df_X.to_dask_array(lengths = True)\n",
    "\n",
    "# df_X_array = df_X_array.rechunk(chunks=(10000,-1))\n",
    "# df_X_array.to_zarr('df_X_array.zarr' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_y_array = df_y.to_dask_array(lengths = True)\n",
    "\n",
    "# df_y_array = df_y_array.rechunk(chunks=(10000,-1))\n",
    "# df_y_array.to_zarr('df_y_array.zarr' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_X_array.to_hdf5('df_X_array.hdf5' , '/x')  \n",
    "# df_y_array.to_hdf5('df_y_array.hdf5' , '/x')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df_X, df_y, df_X_array, df_y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_y = df_profiles[y_columns].compute()\n",
    "\n",
    "# df_X = df_profiles[list(x['selected'])] ## .drop(labels=x_columns_drop, axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read zarr files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:51:02.679359Z",
     "start_time": "2023-12-21T20:51:02.611995Z"
    }
   },
   "outputs": [],
   "source": [
    "df_X_array = dask.array.from_zarr('df_X_array.zarr' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T20:51:03.527251Z",
     "start_time": "2023-12-21T20:51:03.499858Z"
    }
   },
   "outputs": [],
   "source": [
    "df_y_array = dask.array.from_zarr('df_y_array.zarr' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:45:10.550334Z",
     "start_time": "2023-07-31T18:45:07.868769Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "from torchinfo import summary\n",
    "torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=180, profile=None, sci_mode=None)\n",
    "## Set visible GPU device \n",
    "##----------------------------------------------\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:49:05.754561Z",
     "start_time": "2023-07-31T18:49:05.702444Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(val_steps=50):\n",
    "    loss = 0\n",
    "    for i in range(val_steps):\n",
    "        ix = torch.randint(0, val_X.shape[0], (batch_size,))\n",
    "        Xv, Yv = torch.Tensor(val_X[ix]).to(device), torch.Tensor(val_y[ix]).to(device) # batch X,Y\n",
    "        logits = model(Xv)\n",
    "        loss += F.mse_loss(logits, Yv)\n",
    "    loss /= val_steps\n",
    "    return loss \n",
    "\n",
    "# import torchmetrics \n",
    "# evaluate the loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "    \"\"\"\n",
    "    compute loss for data split passed (training, validation, or test data)\n",
    "    \"\"\"\n",
    "#     from torch.torcheval.metrics import R2Score    \n",
    "    from torchmetrics.regression import R2Score, PearsonCorrCoef\n",
    "    x_numpy,y_numpy = {\n",
    "    'train': (train_X, train_y),\n",
    "    'val'  : (val_X  , val_y),\n",
    "    'test' : (test_X , test_y),\n",
    "    }[split]\n",
    "    x = torch.Tensor(x_numpy).to(device)\n",
    "    y = torch.Tensor(y_numpy).to(device) \n",
    "    logits = model(x)\n",
    "#     print(f\" size of logits: {logits.shape}   size of y: {y.shape}\")\n",
    "    mse_loss = F.mse_loss(logits, y)\n",
    "    r2score = R2Score().to(device)\n",
    "    pearson = PearsonCorrCoef(num_outputs=1).to(device)\n",
    "    r2_loss = r2score(logits, y) \n",
    "    pearson_loss= pearson(logits.view(-1), y.view(-1))\n",
    "    print(f\"\\n {split:5s} data:   MSE loss: {mse_loss.item():10.4f}    R2 Score: {r2_loss.item():.5f}     Pearson Coeff. {pearson_loss:.4f}\")\n",
    "    \n",
    "@torch.no_grad()\n",
    "def calc_loss(x,y):\n",
    "    logits = model(x)\n",
    "    loss = F.mse_loss(logits, y)\n",
    "    print(y[:20].T)\n",
    "    print(logits[:20].T)\n",
    "    print(f\"Calculated loss:  {loss.item():5e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:45:44.380055Z",
     "start_time": "2023-07-31T18:45:41.073911Z"
    }
   },
   "outputs": [],
   "source": [
    "# hierarchical network\n",
    "#  nn.Linear(n_hidden_2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "# del model\n",
    "\n",
    "n_input = 1032 # the dimensionality of the character embedding vectors\n",
    "n_hidden_1 = 128 # the number of neurons in the hidden layer of the MLP\n",
    "n_hidden_2 = 64 # the number of neurons in the hidden layer of the MLP\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_input , n_hidden_1, bias=False), \n",
    "    nn.BatchNorm1d(n_hidden_1), \n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_hidden_1, n_hidden_2, bias=False), \n",
    "    nn.BatchNorm1d(n_hidden_2), \n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_hidden_2, 1),\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:47:25.981015Z",
     "start_time": "2023-07-31T18:47:25.916290Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(type(model))\n",
    "# print(type(model[-1]))\n",
    "model[-1].__dict__\n",
    "model[-1].weight.shape\n",
    "model[-1].bias.shape\n",
    "model[-1].weight\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "    model[-1].weight *= 0.1 # last layer make less confident\n",
    "model[-1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:47:48.292853Z",
     "start_time": "2023-07-31T18:47:45.651556Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = model.parameters()\n",
    "ttl_nelements = 0\n",
    "for p in parameters:\n",
    "    print(f\"Parm shape: {str(p.shape):35s}    # elements: {p.nelement():8d}    Required gradient calc: {p.requires_grad}\")\n",
    "    ttl_nelements += p.nelement()\n",
    "print(ttl_nelements)\n",
    "\n",
    "print(f\"Total num of parameters: {sum(p.nelement() for p in model.parameters())}\") # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "summary(model, \n",
    "        input_size = (1,1032),\n",
    "        verbose =2, \n",
    "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"params_percent\",  \"mult_adds\",\"trainable\"],\n",
    "        col_width=16,\n",
    "        row_settings=[\"var_names\"],);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:48:04.381774Z",
     "start_time": "2023-07-31T18:48:04.327798Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42); # seed rng for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_step = 0\n",
    "end_step = 200000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:48:09.459977Z",
     "start_time": "2023-07-31T18:48:09.429767Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "init_LR = 1e-3\n",
    "curr_LR = init_LR\n",
    "step_size = 100000\n",
    "start_step = 0\n",
    "end_step  = 200000\n",
    "batch_size = 64\n",
    "lossi = []\n",
    "lossv = []\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=init_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:48:11.261542Z",
     "start_time": "2023-07-31T18:48:11.227374Z"
    }
   },
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma=0.1, last_epoch= -1 , verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:00:14.450630Z",
     "start_time": "2023-07-31T18:49:12.235999Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train();\n",
    "for i in range(start_step, end_step):\n",
    "  \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "    Xb, Yb = torch.Tensor(train_X[ix]).to(device), torch.Tensor(train_y[ix]).to(device) # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    logits = model(Xb)\n",
    "    loss_mse = F.mse_loss(logits, Yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss_mse.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    lossi.append(loss_mse.log10().item())\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # track stats\n",
    "    if i % 1000 == 0:\n",
    "        val_loss_mse = validation()\n",
    "        lossv.append(val_loss_mse.log10().item())\n",
    "        print(f\" {datetime.now().strftime('%X.%f')} | batch: {i:7d}/{end_step:7d} | training loss: {loss_mse.item():11.6f} | validation loss: {val_loss_mse.item():11.6f}\") \n",
    "    \n",
    "        if i % 50000 == 0:\n",
    "            if curr_LR != optimizer.param_groups[0]['lr']:\n",
    "                curr_LR = optimizer.param_groups[0]['lr']\n",
    "                print(f\" ===> learning rate adjusted to {curr_LR}\")        \n",
    "            model.eval();\n",
    "            print(f\"**{i//50000} iterations**\")\n",
    "            split_loss('train')\n",
    "            split_loss('val')\n",
    "            split_loss('test')\n",
    "            print()\n",
    "            model.train();\n",
    "\n",
    "## End of training loop\n",
    "        \n",
    "print(f\" start_step : {start_step}     end_step: {end_step}    i: {i}\")    \n",
    "\n",
    "model.eval();\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "split_loss('test')\n",
    "model.train();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T10:55:45.256653Z",
     "start_time": "2023-06-04T10:55:45.195469Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start_step = end_step\n",
    "# end_step += 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:52:14.542283Z",
     "start_time": "2023-07-31T19:52:14.519732Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\" start_step : {start_step}     end_step: {end_step}     end_step: {end_step - start_step}   i: {i}      learning rate: {optimizer.param_groups[0]['lr']}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T11:35:15.434870Z",
     "start_time": "2023-06-04T11:35:15.344699Z"
    }
   },
   "outputs": [],
   "source": [
    "# for g in optimizer.param_groups:\n",
    "#     g['lr']=1e-4\n",
    "#     print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T11:35:26.494885Z",
     "start_time": "2023-06-04T11:35:25.936385Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(lossi), len(lossv));\n",
    "# print(lossi[0], lossi[-1])\n",
    "# lossi.pop()\n",
    "# print(len(lossi), len(lossv));\n",
    "# print(lossi[0], lossi[-1])\n",
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));\n",
    "plt.plot(torch.tensor(lossv).view(-1));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics:~:text=The%20r2_score%20function%20computes,score%20of%200.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T10:31:05.356460Z",
     "start_time": "2023-06-04T10:31:04.836269Z"
    }
   },
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for batchnorm especially)\n",
    "model.eval();\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "split_loss('test')\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training using TPSA**\n",
    "\n",
    "     train data:        MSE loss:   125.6541    R2 Score: 0.88529     Pearson Coeff. 0.9419\n",
    "\n",
    "     val   data:        MSE loss:   290.9020    R2 Score: 0.67287     Pearson Coeff. 0.8236\n",
    "\n",
    "     test  data:        MSE loss:   384.7767    R2 Score: 0.67663     Pearson Coeff. 0.8230\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax TPSA 128 x 64 , no BatchNorm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMax TPSA 128 x 64 , no BatchNorm**\n",
    "\n",
    "**50k iterations**\n",
    "\n",
    "     train data:        MSE loss:     0.0001    R2 Score: 0.79670     Pearson Coeff. 0.8926\n",
    "\n",
    "     val   data:        MSE loss:     0.0001    R2 Score: 0.68855     Pearson Coeff. 0.8333\n",
    "\n",
    "     test  data:        MSE loss:     0.0002    R2 Score: 0.66196     Pearson Coeff. 0.8145\n",
    "\n",
    "\n",
    "**100k iterations**\n",
    "\n",
    "     train data:        MSE loss:     0.0001    R2 Score: 0.88567     Pearson Coeff. 0.9418\n",
    "\n",
    "     val   data:        MSE loss:     0.0001    R2 Score: 0.70376     Pearson Coeff. 0.8402\n",
    "\n",
    "     test  data:        MSE loss:     0.0002    R2 Score: 0.70878     Pearson Coeff. 0.8433\n",
    "      \n",
    "      \n",
    "**150k iterations**\n",
    "\n",
    "     train data:        MSE loss:     0.0001    R2 Score: 0.88960     Pearson Coeff. 0.9438\n",
    "\n",
    "     val   data:        MSE loss:     0.0001    R2 Score: 0.70221     Pearson Coeff. 0.8394\n",
    "\n",
    "     test  data:        MSE loss:     0.0002    R2 Score: 0.70636     Pearson Coeff. 0.8420\n",
    "      \n",
    "      \n",
    "**200k iterations**\n",
    "\n",
    "    train data:        MSE loss:     0.0001    R2 Score: 0.89114     Pearson Coeff. 0.9441\n",
    "\n",
    "     val   data:       MSE loss:     0.0001    R2 Score: 0.70325     Pearson Coeff. 0.8394\n",
    "\n",
    "     test  data:       MSE loss:     0.0002    R2 Score: 0.70603     Pearson Coeff. 0.8416\n",
    "      \n",
    "\n",
    "**250k iterations**      \n",
    "      \n",
    "     train data:        MSE loss:     0.0001    R2 Score: 0.89125     Pearson Coeff. 0.9440\n",
    "\n",
    "     val   data:        MSE loss:     0.0001    R2 Score: 0.70360     Pearson Coeff. 0.8398\n",
    "\n",
    "     test  data:        MSE loss:     0.0002    R2 Score: 0.70624     Pearson Coeff. 0.8417  \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax TPSA 128 x 64 , With BatchNorm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMax TPSA 128 x 64 , with BatchNorm**\n",
    "\n",
    "**50k iterations**\n",
    "\n",
    "     train data:    MSE loss:     0.0001    R2 Score: 0.80766     Pearson Coeff. 0.8994\n",
    "\n",
    "     val   data:    MSE loss:     0.0001    R2 Score: 0.66401     Pearson Coeff. 0.8181\n",
    "\n",
    "     test  data:    MSE loss:     0.0002    R2 Score: 0.68471     Pearson Coeff. 0.8312\n",
    "\n",
    "\n",
    "**100k iterations**\n",
    "\n",
    "\n",
    "     train data:    MSE loss:     0.0001    R2 Score: 0.85963     Pearson Coeff. 0.9295\n",
    "\n",
    "     val   data:    MSE loss:     0.0001    R2 Score: 0.68456     Pearson Coeff. 0.8308\n",
    "\n",
    "     test  data:    MSE loss:     0.0002    R2 Score: 0.69034     Pearson Coeff. 0.8358      \n",
    "      \n",
    "      \n",
    "**150k iterations**\n",
    "\n",
    "\n",
    "     train data:    MSE loss:     0.0001    R2 Score: 0.87261     Pearson Coeff. 0.9345\n",
    "\n",
    "     val   data:    MSE loss:     0.0001    R2 Score: 0.69643     Pearson Coeff. 0.8354\n",
    "\n",
    "     test  data:    MSE loss:     0.0002    R2 Score: 0.69435     Pearson Coeff. 0.8362\n",
    "\n",
    "      \n",
    "**200k iterations**\n",
    "\n",
    "\n",
    "     train data:    MSE loss:     0.0001    R2 Score: 0.87318     Pearson Coeff. 0.9345\n",
    "\n",
    "     val   data:    MSE loss:     0.0001    R2 Score: 0.69366     Pearson Coeff. 0.8337\n",
    "\n",
    "     test  data:    MSE loss:     0.0002    R2 Score: 0.69301     Pearson Coeff. 0.8363      \n",
    "     \n",
    "\n",
    "**250k iterations**      \n",
    "      \n",
    "     train data:    MSE loss:     0.0001    R2 Score: 0.87323     Pearson Coeff. 0.9346\n",
    "\n",
    "     val   data:    MSE loss:     0.0001    R2 Score: 0.69520     Pearson Coeff. 0.8343\n",
    "\n",
    "     test  data:    MSE loss:     0.0002    R2 Score: 0.69483     Pearson Coeff. 0.8368\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T13:08:43.483858Z",
     "start_time": "2023-05-29T13:08:43.470308Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "for i in get_scorer_names() :\n",
    "    if \"error\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# minibatch construct\n",
    "ix = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "Xb, Yb = torch.Tensor(train_X[ix]).to(device), torch.Tensor(train_y[ix]).to(device) # batch X,Y\n",
    "\n",
    "print(ix)\n",
    "logits = model(Xb)\n",
    "with torch.no_grad():\n",
    "    a,b = logits.cpu().numpy().squeeze(), Yb.cpu().numpy().squeeze()\n",
    "    print(a.shape, b.shape)\n",
    "    for i, j in zip(logits.cpu().numpy(), Yb.cpu().numpy()):\n",
    "        print(f\" {i[0]:.5f}     {j[0]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Generate Data Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T11:24:30.737468Z",
     "start_time": "2023-06-01T11:24:30.731853Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df_labels, title=\"Profiling Report\")\n",
    "# profile_report = df_labels.profile(html={\"style\": {\"full_width\": True}})\n",
    "# profile.to_file(\"./output/example.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profile widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T11:24:21.411239Z",
     "start_time": "2023-06-01T11:24:21.407376Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profile notebook iframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T11:24:15.511339Z",
     "start_time": "2023-06-01T11:24:15.507281Z"
    }
   },
   "outputs": [],
   "source": [
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create label file `all_y1` from `df_tpsa[TPSA]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:30:00.566696Z",
     "start_time": "2023-05-31T10:30:00.543365Z"
    }
   },
   "outputs": [],
   "source": [
    "# del tpsa_norm, tpsa_numpy,tpsa_numpy_1, normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Normalize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:43:56.178606Z",
     "start_time": "2023-07-31T18:43:56.137230Z"
    }
   },
   "outputs": [],
   "source": [
    "# tpsa_numpy =df_labels['log10TPSA'].to_numpy().reshape(-1,1)\n",
    "tpsa_numpy =df_labels['TPSA'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "print(f\" {tpsa_numpy.shape}  min:  {tpsa_numpy.min()},   max: {tpsa_numpy.max()},  mean: {tpsa_numpy.mean():.4f}, std dev: {tpsa_numpy.std():.4f}\")\n",
    "print(\"           \",tpsa_numpy[:10].T)\n",
    "\n",
    "# normalize function \n",
    "print(f\"\\n Using normalize function\")\n",
    "print(f\" --------------------------\")\n",
    "normalizer = Normalizer()\n",
    "tpsa_norm  = normalizer.fit_transform(tpsa_numpy)\n",
    "print(f\" Parameters: {normalizer.get_params(deep=True)}\")\n",
    "# # all_y = normalizer.transform(tpsa_numpy)\n",
    "print(f\" {tpsa_norm.shape}  min:  {tpsa_norm.min()},   max: {tpsa_norm.max():.4f},  mean: {tpsa_norm.mean():.4f}, std dev: {tpsa_norm.std():.4f}\")\n",
    "print(f\" First 10 elements : {tpsa_norm[:10].T}\")      \n",
    "\n",
    "print(f\"\\n Using MaxAbsScaler \")\n",
    "print(f\" --------------------------\")\n",
    "scaler  = MaxAbsScaler()\n",
    "print(f\" type of scaler: {type(scaler)}  \")\n",
    "tpsa_maxabs  = scaler.fit_transform(tpsa_numpy)\n",
    "print(f\" Parameters: {scaler.get_params(deep=True)}\")\n",
    "# # all_y = normalizer.transform(tpsa_numpy)\n",
    "print(f\" {tpsa_maxabs.shape}  min:  {tpsa_maxabs.min()},   max: {tpsa_maxabs.max():.4f},  mean: {tpsa_maxabs.mean():.4f}, std dev: {tpsa_maxabs.std():.4f}\")\n",
    "print(f\"  First 10 elements : {tpsa_maxabs[:10].T}\")      \n",
    "\n",
    "print(f\"\\n Using MinMaxScaler \")\n",
    "print(f\" --------------------------\")\n",
    "scaler  = MinMaxScaler(feature_range=(-1,1))\n",
    "print(f\" type of scaler: {type(scaler)}\")\n",
    "print(f\" Parameters: {scaler.get_params(deep=True)}\")\n",
    "tpsa_minmax  = scaler.fit_transform(tpsa_numpy)\n",
    "print(f\" {tpsa_minmax.shape}  min:  {tpsa_minmax.min()},   max: {tpsa_minmax.max():.4f},  mean: {tpsa_minmax.mean():.4f}, std dev: {tpsa_minmax.std():.4f}\")\n",
    "print(f\" First 10 elements : {tpsa_minmax[:10].T}\")      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:44:20.423785Z",
     "start_time": "2023-07-31T18:44:20.365062Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_y = tpsa_numpy.copy()\n",
    "all_y = tpsa_minmax.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:44:51.753335Z",
     "start_time": "2023-07-31T18:44:51.720475Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\" {all_y.shape} -   min:  {all_y.min()}       max: {all_y.max():.4f}      mean: {all_y.mean():.4f}      std dev: {all_y.std():.4f}\")\n",
    "print(f\" First 10 elements : {all_y[:10].T}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read fingerprint features file `all_X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:44:57.902905Z",
     "start_time": "2023-07-31T18:44:57.871120Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\" compound Pharmacophore Dense Zipped File : {compoundPharmacophoreDenseZipFile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:44:58.568191Z",
     "start_time": "2023-07-31T18:44:58.170427Z"
    }
   },
   "outputs": [],
   "source": [
    "all_X = np.load(compoundPharmacophoreDenseZipFile)['dense_mat']\n",
    "print(type(all_X), all_X.shape, all_X.dtype)\n",
    "print(all_X[:5, :25])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cp311]",
   "language": "python",
   "name": "conda-env-cp311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "380.631px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
