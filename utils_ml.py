'''
date: 2019-10-02
author: Xinhao
'''

import pandas as pd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import itertools
from pprint import pprint
import joblib
import torch
import statistics

from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder, LabelBinarizer
from sklearn.base import clone
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_absolute_error , mean_squared_error, r2_score
from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score, matthews_corrcoef
from sklearn.model_selection._split import check_cv
from sklearn.model_selection import KFold, cross_validate, GridSearchCV, cross_val_score, RandomizedSearchCV, cross_val_predict

# time.strftime(' %x%X')
# datetime.now().strftime('%X.%f')
# time.strftime('%X %x %Z')
# print(datetime.now().strftime('%D-%X.%f'))
# time_fmt = '%Y-%M-%d %H:%m:%S.%f'
# print(datetime.now().strftime(time_fmt)) 


# can also be found in descriptor_selcetions.ipynb
def feature_selection(df, nonzero_thrd = 0.0, cor_thrd = 0.95):
    '''
    remove the zero variance and highly correlated features
    
    df: train features
    
    '''
    selector = VarianceThreshold(nonzero_thrd)
    selector.fit(df)
    nonzero_df = df[df.columns[selector.get_support(indices=True)]]
    
    #remove high correlated features
    ## Create correlation matrix
    corr_matrix = nonzero_df.corr().abs()
    # Select upper triangle of correlation matrix
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
    
    # Find index of feature columns with correlation greater than 0.95
    to_drop = [column for column in upper.columns if any(upper[column] > cor_thrd)]
    
    return nonzero_df.drop(nonzero_df[to_drop], axis=1)

def feature_norm_fit(train_df , scaler = MinMaxScaler()):
    '''
    fit and transform the training data.
    
    train_df: training data
    
    scaler: return the scaler which will be used for test set.
    '''
    array =  train_df.values
    df_norm = pd.DataFrame(scaler.fit_transform(array), columns=train_df.columns, index=train_df.index)
    return df_norm, scaler

def feature_norm_transform(test_df, scaler):
    '''
    normilize the test set according to the scaler generated by the training data.
    
    test_df: test features
    scaler: fitted scaler
    '''
    array =  test_df.values
    df_norm = pd.DataFrame(scaler.transform(array), columns=test_df.columns, index=test_df.index)
    return df_norm  

def test_feature(df, feature, scaler = None):
    '''
    transform the raw (computed) feature into the fowmat ready for modeling.
    
    df: test features
    scaler: for rdkit2d and mordred
    feature: name of the feature ['ecfp6_bits', 'ecfp6_counts', 'maccs', 'rdkit2d', 'mordred']
    '''
    
    with open('../data/Descriptors/filtered_features.json') as f:
        dict_features = json.load(f)
        
    if feature not in dict_features.keys():
        raise Exception(f'The feature **{feature}** is not support, please choose from [ecfp6_bits, ecfp6_counts, maccs, rdkit2d, mordred]')
        
    filtered_desc = dict_features[feature]
    df = df[filtered_desc]
    
    if scaler:
        df = feature_norm_transform(df, scaler)
    
    return df


def prepare_input(df_label, df_feature, target, encoder = None):
    '''
    Take the dataframes of labels and features and prepare them for modeling
    input:
        df_label:    training labels 
        df_features: training data
        target:      column name of label/endpoint to model 
        encoder:     label encoder (for classification models only)
    
    Output:
        df_labeled:        df of non-null labels
        df_unlabeled:      df of null labels 
        labeled_Y L        values of target column 
        labeled_feature:   training samples with a non-null label
        ublabeled_feature: training samples with a null label
        
        
    '''
    # Remove training examples that are missing their labels
    # get the dataframes for labeled and unlabled data from the target.
    
    df_labeled = df_label[~df_label[target].isnull()]
    df_unlabled = df_label[df_label[target].isnull()]
    
    print(f" df_labeled size: {df_labeled.shape}    df_unlabled : {df_unlabled.shape}")
    
    # feature list
    labeled_feature = df_feature.loc[df_labeled.index].values.astype('float32')
    ublabeled_feature = df_feature.loc[df_unlabled.index].values.astype('float32')
    
    labeled_Y = df_labeled[target].values

    if encoder:
        labeled_Y = encoder.transform(labeled_Y)

    return labeled_feature, ublabeled_feature, labeled_Y, df_labeled.index, df_unlabled.index


# model selection (hyperparameter tuning)

def model_selection(model, params_grid, X, y, 
                    scoring = None, cv=5, n_jobs=6, GridSearch = True, n_iter=20, 
                    refit = True, verbose = 0):
    '''
    return the refitted model on the whole train data
    input:
    -----
    model:          model to be grid searched 
    params_grid:    dictionary of hyperparms to grid search 
    X, y:           obvious
    scoring:        scoring Strategy to evaluate the performance of the cross-validated model on the test set.
    cv:             cross-validation fold, integer specifies the number of folds in a (Stratified)KFold,
                    stratified is used if the estimator is a classifier and y is either binary or multiclass
    n_iter          Number of parameter settings that are sampled in RandomizedSearchCV.
                    n_iter trades off runtime vs quality of the solution.
    refit:          Refit an estimator using the best found parameters on the whole dataset.
    
    '''
    print(f" verbose is {verbose}")
    if verbose > 0 : 
        print(f" params_grid: {params_grid}")
        print(f" scoring    : {scoring}     cv:  {cv}   n_jobs: {n_jobs}   gridsearch: {GridSearch}    n_iter: {n_iter}   refit: {refit}   ")
    if GridSearch == True:
        model_train = GridSearchCV(model, params_grid, cv=cv, n_jobs=n_jobs, scoring = scoring, 
                                   refit = refit, verbose = verbose)
    else:
        model_train = RandomizedSearchCV(model, param_distributions = params_grid, 
                                         n_iter = n_iter,cv=cv, n_jobs=n_jobs, scoring=scoring, 
                                         refit = refit, verbose = verbose)
    
    model_train.fit(X, y)
    
    print("Best parameters set found on development set:", model_train.best_params_ )
    print("Best score:", model_train.best_score_ )
    
    print("Grid scores on development set:")
    print()
    means = model_train.cv_results_['mean_test_score']
    stds = model_train.cv_results_['std_test_score']


    for mean, std, params in zip(means, stds, model_train.cv_results_['params']):
        print("%0.5f (+/-%0.03f) for %r" % (mean, std * 2, params))
    return model_train


# Hfeatures
def Regression_meta_features(model, X, y, NaN, 
                 index_X, index_NaN, col_names = ['RM_pred'], cv=10, n_jobs = 6):
    '''
    ***generate meta_features used to train the meta models***
    input: 
    model
    X:         Valid training samples
    y:         Training samples with an valid value in the label column 
    Unlabled   Training samples with an invalid value in the target(label) column
    Index_X:   Indicies of valid training samples
    index_Nan: indicies of invalid training examples
    
    process:
    1. Get out-of-fold predictions (cross-validation) of labeled data.
    2. train the model based on labeled data and then make predictions on un-labeled data.
    
    return:
    1. meta_features used to train the meta models
    2. off predictions to measure the cv performance of the trianed base model
    3. the base model trained on the whole labeled data
    
    '''
    regression_scoring = {'RMSE': make_scorer(rmse), 
                          'R2'  : 'r2', 
                          'MAE' : make_scorer(mean_absolute_error),
                          'MSE' : make_scorer(mean_squared_error)}
    
    np.random.seed(1234) # get reproducible results
    
    ## KFold : Provides train/test indices to split data in train/test sets. 
    ##         Split dataset into k consecutive folds (without shuffling by default).
    ##         Each fold is then used once as a validation while the k - 1 remaining folds form the training set.
    
    kfold = KFold(n_splits=cv, shuffle=True, random_state=15)       
    
    ## Clone passed model and fit on labeled data
    instance = clone(model)
    instance.fit(X, y)
    
    ## make predictions on unlabeld data using fitted model 
    NaN_fold_predictions = instance.predict(NaN)   
    
    ## make predictions on out_of_fold labeled data
    ## The data is split according to the cv parameter. Each sample belongs to exactly one test set, 
    ## and its prediction is computed with an estimator fitted on the corresponding training set.
    
    out_of_fold_predictions = cross_val_predict(model, X, y, cv=kfold, method = 'predict', n_jobs = n_jobs)
    
    ## cross_validate() vs. cross_val_score()  function:
    ## - It allows specifying multiple metrics for evaluation.
    ## - It returns a dict containing fit-times, score-times 
    ##   (and optionally training scores as well as fitted estimators) in addition to the test score.
    
    cv_score = cross_validate(model, X, y, cv=kfold, n_jobs = n_jobs, scoring = regression_scoring)
    
    OOF_predictions = pd.DataFrame(out_of_fold_predictions, index = index_X, columns=col_names)     
    NaN_predictions = pd.DataFrame(NaN_fold_predictions, index = index_NaN, columns=col_names)
    meta_feature = pd.concat([OOF_predictions, NaN_predictions])

#     RM_mf, RM_oof, RM_base_model, cv_score
    return meta_feature, out_of_fold_predictions, instance, cv_score


def Classification_meta_features(model,X,y,NaN, 
                 index_X, index_NaN, col_names, cv=10, Probs=True, n_jobs = 6):
    '''
    Get meta-feautes of out-of-fold and predcion-fold data.
    
    method: For method=’predict_proba’, the columns correspond to the classes in sorted order.
    
    Return:
    
    cv_score : dictionary containing fit_time, score_time, test_Accuracy, test_Balance Accuracy
                                     test_matthews_corrcoef, test_f1_score, test_AUROC for each fold
    instance : clone of model
    
    out_of_fold_predictions:
    
    meta_feature :  Out of Fold predictions as a dataframe     
    '''
    classification_scoring = {'Accuracy'         : make_scorer(accuracy_score), 
                              'Balance Accuracy' : make_scorer(balanced_accuracy_score), 
                              'matthews_corrcoef': make_scorer(matthews_corrcoef),
                              'f1_score'         : make_scorer(f1_score, average='weighted'),
                              'AUROC'            : make_scorer(multiclass_roc_auc_score)    
                             }
    
    np.random.seed(1234) # get reproducible results
    ##
    kfold = KFold(n_splits=cv, shuffle=True, random_state=15)    
    
    ## Clone does a deep copy of the model in an estimator without actually copying attached data. 
    ## It returns a new estimator with the same parameters that has not been fitted on any data.
    instance = clone(model)
    instance.fit(X, y)
        
    if Probs == True:
        NaN_fold_predictions = instance.predict_proba(NaN)
        out_of_fold_predictions = cross_val_predict(model, X, y, cv=kfold, method = 'predict_proba', n_jobs = n_jobs)
        
        cv_score = cross_validate(model, X, y, cv=kfold, n_jobs = n_jobs, scoring = classification_scoring)
        
        OOF_predictions = pd.DataFrame(out_of_fold_predictions, index = index_X, columns=col_names)     
        NaN_predictions = pd.DataFrame(NaN_fold_predictions, index = index_NaN, columns=col_names)
    
        meta_feature = pd.concat([OOF_predictions, NaN_predictions])
            
    else: 
        NaN_fold_predictions = instance.predict(NaN)        
        out_of_fold_predictions = cross_val_predict(model, X, y, cv=kfold, method = 'predict', n_jobs = n_jobs)
        
        cv_score = cross_validate(model, X, y, cv=kfold, n_jobs = n_jobs, scoring = classification_scoring)
        
        OOF_predictions = pd.DataFrame(out_of_fold_predictions, index = index_X, columns=col_names)     
        NaN_predictions = pd.DataFrame(NaN_fold_predictions, index = index_NaN, columns=col_names)
        meta_feature = pd.concat([OOF_predictions, NaN_predictions])
        
    return meta_feature, out_of_fold_predictions, instance, cv_score

def result_model_selection(results, name):
    df_results = pd.DataFrame({'model'     : [name] * len(results.cv_results_['params']),
                               'params'    : results.cv_results_['params'],
                               'mean score': results.cv_results_['mean_test_score'],
                               'std score' : results.cv_results_['std_test_score'],
                               'rank'      : results.cv_results_['rank_test_score']
                              })
    return df_results

def rmse(y_true, y_pred):
    rmse = math.sqrt(mean_squared_error(y_true, y_pred))
    return rmse

def report_reg_models(score):
    print('RMSE:', rmse(y_true, y_pred))
    print('R2:', r2_score(y_true, y_pred))
    print('MAE', mean_absolute_error(y_true, y_pred))
    print('MSE', mean_squared_error(y_true, y_pred))

def report_reg_scores(labels, predictions):
    '''
    two dataframe: label, and predictions
    
    report four scores: RMSE, R2, MAE, MSE
    '''
    
    # get the labeled data
    labeled = labels[~labels['logLD50_mmolkg'].isnull()]
    
    labeled_preds = predictions.loc[labeled.index].values.astype('float32')
    labeled_Y = labeled['logLD50_mmolkg'].values
    
    score_rmse = rmse(labeled_Y, labeled_preds)
    score_r2 = r2_score(labeled_Y, labeled_preds)
    score_mae = mean_absolute_error(labeled_Y, labeled_preds)
    score_mse = mean_squared_error(labeled_Y, labeled_preds)

    print('RMSE:', score_rmse)
    print('R2:', score_r2) 
    print('MAE:', score_mae)
    print('MSE:', score_mse)
    
    return [score_rmse, score_r2, score_mae, score_mse]  


def prob_to_pred(probs):
    classes = probs.argmax(axis=-1)
    return classes


def report_cv_reg_models(score, decimal = 3):
    print('RMSE:', round(statistics.mean(score['test_RMSE']), decimal), 'std:',
                   round(statistics.stdev(score['test_RMSE']), decimal))

    print('R2:' , round(statistics.mean(score['test_R2']), decimal), 'std:',
                  round(statistics.stdev(score['test_R2']), decimal))
    
    print('MAE:', round(statistics.mean(score['test_MAE']), decimal), 'std:',
                  round(statistics.stdev(score['test_MAE']), decimal))
    
    print('MSE:', round(statistics.mean(score['test_MSE']), decimal), 'std:',
                  round(statistics.stdev(score['test_MSE']), decimal))

def multiclass_roc_auc_score(y_test, y_pred, average="weighted"):
    '''
    this function is from 
    https://medium.com/@plog397/auc-roc-curve-scoring-function-for-multi-class-classification-9822871a6659
    '''
    lb = LabelBinarizer()
    lb.fit(y_test)
    y_test = lb.transform(y_test)
    y_pred = lb.transform(y_pred)
    return roc_auc_score(y_test, y_pred, average=average)


def report_clf_models(score, decimal = 3):
    print('Accuracy:', round(statistics.mean(score['test_Accuracy']), decimal), 'std:',
                   round(statistics.stdev(score['test_Accuracy']), decimal))

    print('Balance Accuracy:', round(statistics.mean(score['test_Balance Accuracy']), decimal), 'std:',
                   round(statistics.stdev(score['test_Balance Accuracy']), decimal))
    
    print('matthews_corrcoef:', round(statistics.mean(score['test_matthews_corrcoef']), decimal), 'std:',
                   round(statistics.stdev(score['test_matthews_corrcoef']), decimal))
    
    print('f1_score:', round(statistics.mean(score['test_f1_score']), decimal), 'std:',
                   round(statistics.stdev(score['test_f1_score']), decimal))

    print('AUROC:', round(statistics.mean(score['test_AUROC']), decimal), 'std:',
                   round(statistics.stdev(score['test_AUROC']), decimal))


def report_clf_scores(labels, predictions, target, encoder):
    '''
    two dataframe: label, and predictions
    target: ['toxic', 'EPA_category']
    encoder: label encoder
    
    report four scores: Accuracy, Balance Accuracy, MCC, f1_weight, AUROC (Only for binary model)
    '''
    # get the labeled data
    labeled = labels[~labels[target].isnull()]
    
    labeled_probs = predictions.loc[labeled.index].values.astype('float32')
    # predicted probabilities to predicted class
    labeled_preds = prob_to_pred(labeled_probs)
    
    # label encoding
    labeled_Y = encoder.transform(labeled[target].values)
    
    accuracy = accuracy_score(labeled_Y, labeled_preds)
    balance_acc = balanced_accuracy_score(labeled_Y, labeled_preds)
    f1= f1_score(labeled_Y, labeled_preds, average='weighted')
    mcc = matthews_corrcoef(labeled_Y, labeled_preds)
    
    score_list = [accuracy, balance_acc, f1, mcc]
    
    print('Accuracy:', accuracy)
    print('Balance Accuracy:', balance_acc) 
    print('F1_score:', f1)
    print('MCC:', mcc)
    
    if target == 'toxic':
        auroc = roc_auc_score(labeled_Y, labeled_preds)
        print('AUROC:', auroc)
        score_list.append(auroc)
        
    return score_list

 
